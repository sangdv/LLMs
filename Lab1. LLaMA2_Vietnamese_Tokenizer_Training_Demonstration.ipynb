{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Install Some Libraries"
      ],
      "metadata": {
        "id": "mBtyeVk0Fj0W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -qq install datasets sentencepiece"
      ],
      "metadata": {
        "id": "Q8sB1ru8FltX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Preparation"
      ],
      "metadata": {
        "id": "h3cPX6epEecB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download dataset [news-corpus](https://github.com/binhvq/news-corpus) from binhvq\n",
        "\n",
        "Datasets containing full Vietnamese news until 21/05/2021\n",
        "\n",
        "We will download partially pre-processed, publicly available this dataset from [Huggingface](https://huggingface.co/datasets/vietgpt/binhvq_news_vi)"
      ],
      "metadata": {
        "id": "SR87eU6OEgME"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aWtfXxoqBSLX",
        "outputId": "4a5832ec-3940-4691-e784-c253e6eef1d0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "HUGGINGFACE_PATH = \"tdtunlp/binhvq_news_vi\"\n",
        "NUM_CORE = 2 # Colab Maximum Core\n",
        "\n",
        "dataset = load_dataset(\n",
        "    path=HUGGINGFACE_PATH,\n",
        "    data_files=\"data/train-00000-of-00009-848cb14e692f7fe1.parquet\",\n",
        "    num_proc=NUM_CORE,\n",
        "    split=\"train\",\n",
        "    verification_mode=\"no_checks\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eXmj_Am8IImc",
        "outputId": "1611a8c5-be9d-4061-f8d0-b615e3cc9df5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['text'],\n",
              "    num_rows: 2151733\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset[:4], # data looks clean!"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PBM3VXJJFc-S",
        "outputId": "5968f140-6dae-4fbf-8741-cd280d659306"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "({'text': ['Sau đó, doanh nghiệp này điều chỉnh giá đi xuống nhưng vẫn cao hơn hôm trước 40.000 đồng/lượng, ở mức 36,86 triệu đồng/lượng-36,96 triệu đồng/lượng đối với giao dịch lẻ và 36,87 triệu đồng/lượng-36,95 triệu đồng/lượng đối với giao dịch buôn.',\n",
              "   'Đội hình thi đấu 2 đội: AC Milan: Abbiati, Nesta (Bonera), Zambrotta, Abate, Silva, Van Bommel (Noceniro), Seedorf, Aquilani, Boateng, Ibrahimovic, Robinho (Pato) Barcelona: Valdes, Puyol, Abidal, Fabregas (Pedro), Xavi, Thiago (Dos Santos), Mascherano, Keita, Busquets, Villa (Sanchez), Messi Chính Nghĩa.',\n",
              "   'Nguyễn Hà My Email: vonga_kachiusa@yahoo.com ĐC: Phòng 11, số nhà 434/7 đường Bình Quới, phường 28, quận Bình Thạnh, TP.HCM ĐT: 0985 485 379 Bài: ĐT Ba Lan và những giấc mơ hiển hiện (http://thethao.vietnamnet.vn/vn/euro/ban-doc/14733/dt-ba-lan-va-nhung-giac-mo-hien-hien.html ) Ngày đăng: 08/06/2012 01:51 5.',\n",
              "   'ĐỘI HÌNH THI ĐÂÚChelsea: Courtois, Azpilicueta, Luiz, Cahill, Moses (Zouma, 85 ), Kante, Fabregas, Alonso, Pedro (Matic, 76 ), Costa, Hazard (Willian, 85 ).Swansea: Fabianski, Naughton, Fernandez, Mawson, Olsson, Fer, Cork, Carroll (Ayew, 76 ), Routledge (Narsingh, 81 ), Llorente, Sigurdsson.']},)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will select random 100,000 docs to train the tokenizer"
      ],
      "metadata": {
        "id": "ZNEZHMbfIfAZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "NUM_SAMPLES = 100_000\n",
        "\n",
        "sub_dataset = dataset.shuffle(seed=42)[:NUM_SAMPLES]"
      ],
      "metadata": {
        "id": "YpxGDgZpIK2a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Converting to text file (.txt)\n",
        "as the `sentencepiece` library input raw file with line-by-line text.\n",
        "\n",
        "More details on https://github.com/google/sentencepiece?tab=readme-ov-file#train-sentencepiece-model\n"
      ],
      "metadata": {
        "id": "RD754PISHeT1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "OUTPUT_TXT_FILE_PATH = \"raw_docs.txt\"\n",
        "\n",
        "with open(OUTPUT_TXT_FILE_PATH, \"w\") as fOut:\n",
        "    for sample in tqdm(dataset):\n",
        "        fOut.write(sample['text'].strip() + \"\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vU67MIV-G9au",
        "outputId": "d8f04f56-10b2-4610-c60c-c001a914e1da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2151733/2151733 [01:22<00:00, 26083.49it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training Vietnamese Tokenizer with Sentencepiece"
      ],
      "metadata": {
        "id": "-fZ08pADKBOI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sentencepiece as spm"
      ],
      "metadata": {
        "id": "HJNtrZm9J_NT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "INPUT_DATA_FILES = [OUTPUT_TXT_FILE_PATH] # support mutitples paths\n",
        "MODEL_PREFIX= \"vietnamese_sp\"\n",
        "VOCAB_SIZE = 20_000\n",
        "\n",
        "# About 200 seconds\n",
        "# Llama2 tokenizer training settings\n",
        "## https://github.com/karpathy/llama2.c/blob/master/doc/train_llama_tokenizer.md\n",
        "spm.SentencePieceTrainer.Train(\n",
        "    input=INPUT_DATA_FILES,\n",
        "    model_prefix=MODEL_PREFIX,\n",
        "    vocab_size=VOCAB_SIZE,\n",
        "    self_test_sample_size=0,\n",
        "    input_format=\"text\",\n",
        "    num_threads=NUM_CORE,\n",
        "    split_digits=True,\n",
        "    allow_whitespace_only_pieces=True,\n",
        "    byte_fallback=True,\n",
        "    unk_surface=r\" \\342\\201\\207 \",\n",
        "    model_type='bpe',\n",
        "    max_sentence_length=999_999_999, # no maximum\n",
        "    input_sentence_size=200_000_000, # no input sentence\n",
        "    normalization_rule_name=\"identity\",\n",
        "    shuffle_input_sentence=True\n",
        ")\n",
        "\n",
        "# Output .model and .vocab file"
      ],
      "metadata": {
        "id": "rWaJhvJ3KabO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check file `vietnamese_sp.vocab` to see all of the subwords and their corresponding index"
      ],
      "metadata": {
        "id": "U2HJxQrjNwb2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Merge the trained tokenizers to the original Llama2"
      ],
      "metadata": {
        "id": "xnMDR3GIN3gU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall -y protobuf\n",
        "!pip install --no-binary=protobuf protobuf\n",
        "\n",
        "# Click restart sessions"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 532
        },
        "id": "litUP8FCP4Lc",
        "outputId": "fa9f4be2-9f2a-437f-96e1-e607d16bcfbe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: protobuf 3.20.3\n",
            "Uninstalling protobuf-3.20.3:\n",
            "  Successfully uninstalled protobuf-3.20.3\n",
            "Collecting protobuf\n",
            "  Downloading protobuf-5.28.0.tar.gz (422 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m422.4/422.4 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: protobuf\n",
            "  Building wheel for protobuf (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for protobuf: filename=protobuf-5.28.0-cp310-cp310-linux_x86_64.whl size=1100615 sha256=fe204d2a9c9e28354c8546d36040f8e14864ca4ff904dd48c4ae45170135ea90\n",
            "  Stored in directory: /root/.cache/pip/wheels/4b/40/5d/c217f7b0d2b541a79338d11fe750739e3304d5c709629a0338\n",
            "Successfully built protobuf\n",
            "Installing collected packages: protobuf\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "cudf-cu12 24.4.1 requires protobuf<5,>=3.20, but you have protobuf 5.28.0 which is incompatible.\n",
            "google-ai-generativelanguage 0.6.6 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 5.28.0 which is incompatible.\n",
            "google-cloud-bigquery-storage 2.25.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 5.28.0 which is incompatible.\n",
            "google-cloud-datastore 2.19.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 5.28.0 which is incompatible.\n",
            "google-cloud-firestore 2.16.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 5.28.0 which is incompatible.\n",
            "tensorboard 2.17.0 requires protobuf!=4.24.0,<5.0.0,>=3.19.6, but you have protobuf 5.28.0 which is incompatible.\n",
            "tensorflow 2.17.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3, but you have protobuf 5.28.0 which is incompatible.\n",
            "tensorflow-metadata 1.15.0 requires protobuf<4.21,>=3.20.3; python_version < \"3.11\", but you have protobuf 5.28.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed protobuf-5.28.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google"
                ]
              },
              "id": "24d0f93975a24e42a829c4017e8011e0"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION\"]=\"python\"\n",
        "\n",
        "from transformers import LlamaTokenizer\n",
        "from sentencepiece import sentencepiece_model_pb2 as sp_pb2_model\n",
        "import sentencepiece as spm\n"
      ],
      "metadata": {
        "id": "Le1k15KfK3f0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "LLAMA_TOKENIZER_DIR = \"NousResearch/Llama-2-7b-hf\" # Duplicated Meta's version Not require HF token for official meta\n",
        "VIETNAMESE_SP_MODEL_FILE = \"vietnamese_sp.model\" # trained file"
      ],
      "metadata": {
        "id": "mMTg6sa5OWUP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load\n",
        "llama_tokenizer = LlamaTokenizer.from_pretrained(LLAMA_TOKENIZER_DIR)\n",
        "vietnamese_sp_model = spm.SentencePieceProcessor()\n",
        "vietnamese_sp_model.Load(VIETNAMESE_SP_MODEL_FILE)\n",
        "\n",
        "llama_spm = sp_pb2_model.ModelProto()\n",
        "llama_spm.ParseFromString(llama_tokenizer.sp_model.serialized_model_proto())\n",
        "vietnamese_spm = sp_pb2_model.ModelProto()\n",
        "vietnamese_spm.ParseFromString(vietnamese_sp_model.serialized_model_proto())\n",
        "\n",
        "# print number of tokens\n",
        "print(len(llama_tokenizer),len(vietnamese_sp_model))\n",
        "\n",
        "print(llama_tokenizer.all_special_tokens)\n",
        "print(llama_tokenizer.all_special_ids)\n",
        "print(llama_tokenizer.special_tokens_map)\n",
        "\n",
        "## Add Vietnamse tokens to LLaMA tokenizer\n",
        "llama_spm_tokens_set=set(p.piece for p in llama_spm.pieces)\n",
        "\n",
        "\n",
        "print(len(llama_spm_tokens_set))\n",
        "print(f\"Before:{len(llama_spm_tokens_set)}\")\n",
        "for p in vietnamese_spm.pieces:\n",
        "    piece = p.piece\n",
        "    if piece not in llama_spm_tokens_set:\n",
        "        new_p = sp_pb2_model.ModelProto().SentencePiece()\n",
        "        new_p.piece = piece\n",
        "        new_p.score = 0\n",
        "        llama_spm.pieces.append(new_p)\n",
        "print(f\"New model pieces: {len(llama_spm.pieces)}\")\n",
        "\n",
        "## Save\n",
        "output_sp_dir = 'merged_tokenizer_sp' # save as SP format\n",
        "output_hf_dir = 'merged_tokenizer_hf' # the path to save Vietnamese-LLaMA tokenizer as HF format\n",
        "os.makedirs(output_sp_dir,exist_ok=True)\n",
        "with open(output_sp_dir+'/vietnamese_llama.model', 'wb') as f:\n",
        "    f.write(llama_spm.SerializeToString())\n",
        "\n",
        "tokenizer = LlamaTokenizer(vocab_file=output_sp_dir+'/vietnamese_llama.model', legacy=False)\n",
        "\n",
        "tokenizer.save_pretrained(output_hf_dir)\n",
        "print(f\"Vietnamese-LLaMA tokenizer has been saved to {output_hf_dir}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ssA04tlOUqZ",
        "outputId": "669fe8f0-61b7-4030-cb39-f827dd531769"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "32000 20000\n",
            "['<s>', '</s>', '<unk>']\n",
            "[1, 2, 0]\n",
            "{'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<unk>'}\n",
            "32000\n",
            "Before:32000\n",
            "New model pieces: 45937\n",
            "Vietnamese-LLaMA tokenizer has been saved to merged_tokenizer_hf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluate new tokenizers"
      ],
      "metadata": {
        "id": "zGBgzD4yRuEy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Test\n",
        "llama_tokenizer = LlamaTokenizer.from_pretrained(LLAMA_TOKENIZER_DIR)\n",
        "vietnamese_llama_tokenizer = LlamaTokenizer.from_pretrained(output_hf_dir)\n",
        "print(tokenizer.all_special_tokens)\n",
        "print(tokenizer.all_special_ids)\n",
        "print(tokenizer.special_tokens_map)\n",
        "\n",
        "text='''Trái đất tròn quay, mặt trời mọc ngàn dặm xa, đánh thức cuộc sống bình minh. Những cánh diều trắng bay cao trên bầu trời, gió êm đềm thổi qua biển cỏ xanh mơn mởn. Cảm giác hạnh phúc như những bông hoa đua nở trong ánh nắng ấm áp của mùa xuân. Điều quan trọng là ta phải yêu và trân trọng cuộc sống này.\n",
        "The primary use of LLaMA is research on large language models, including'''\n",
        "print(\"Test text:\\n\",text)\n",
        "print(\"---------------\")\n",
        "print(f\"Tokenized by LLaMA tokenizer:{llama_tokenizer.tokenize(text)}\")\n",
        "print(f\"Tokenized by Vietnamese-LLaMA tokenizer:{vietnamese_llama_tokenizer.tokenize(text)}\")\n",
        "print(\"---------------\")\n",
        "print(f\"Tokenized by LLaMA tokenizer IDs: \"  + str(llama_tokenizer(text)[\"input_ids\"]))\n",
        "print(f\"Tokenized by Vietnamese-LLaMA tokenizer IDs: \"  + str(vietnamese_llama_tokenizer(text)[\"input_ids\"]))\n",
        "\n",
        "text='''<unk>\n",
        "\n",
        "'''\n",
        "print(\"Test text:\\n\",text)\n",
        "print(\"---------------\")\n",
        "print(f\"Tokenized by LLaMA tokenizer:{llama_tokenizer.tokenize(text)}\")\n",
        "print(f\"Tokenized by Vietnamese-LLaMA tokenizer:{vietnamese_llama_tokenizer.tokenize(text)}\")\n",
        "print(\"---------------\")\n",
        "print(f\"Tokenized by LLaMA tokenizer IDs: \"  + str(llama_tokenizer(text)[\"input_ids\"]))\n",
        "print(f\"Tokenized by Vietnamese-LLaMA tokenizer IDs: \"  + str(vietnamese_llama_tokenizer(text)[\"input_ids\"]))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c1H3JETkO7qJ",
        "outputId": "5ca74974-5477-4ac0-956d-c00d447a06d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['<s>', '</s>', '<unk>']\n",
            "[1, 2, 0]\n",
            "{'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>'}\n",
            "Test text:\n",
            " Trái đất tròn quay, mặt trời mọc ngàn dặm xa, đánh thức cuộc sống bình minh. Những cánh diều trắng bay cao trên bầu trời, gió êm đềm thổi qua biển cỏ xanh mơn mởn. Cảm giác hạnh phúc như những bông hoa đua nở trong ánh nắng ấm áp của mùa xuân. Điều quan trọng là ta phải yêu và trân trọng cuộc sống này.\n",
            "The primary use of LLaMA is research on large language models, including\n",
            "---------------\n",
            "Tokenized by LLaMA tokenizer:['▁Tr', 'á', 'i', '▁', 'đ', 'ấ', 't', '▁tr', 'ò', 'n', '▁qu', 'ay', ',', '▁m', '<0xE1>', '<0xBA>', '<0xB7>', 't', '▁tr', 'ờ', 'i', '▁m', 'ọ', 'c', '▁ng', 'àn', '▁d', '<0xE1>', '<0xBA>', '<0xB7>', 'm', '▁x', 'a', ',', '▁', 'đ', 'án', 'h', '▁th', 'ứ', 'c', '▁cu', 'ộ', 'c', '▁s', 'ố', 'ng', '▁b', 'ì', 'n', 'h', '▁min', 'h', '.', '▁N', 'h', 'ữ', 'ng', '▁c', 'án', 'h', '▁di', 'ề', 'u', '▁tr', 'ắ', 'ng', '▁bay', '▁ca', 'o', '▁tr', 'ên', '▁b', 'ầ', 'u', '▁tr', 'ờ', 'i', ',', '▁g', 'ió', '▁', 'êm', '▁', 'đ', 'ề', 'm', '▁th', '<0xE1>', '<0xBB>', '<0x95>', 'i', '▁qu', 'a', '▁bi', 'ể', 'n', '▁c', '<0xE1>', '<0xBB>', '<0x8F>', '▁x', 'anh', '▁m', 'ơ', 'n', '▁m', '<0xE1>', '<0xBB>', '<0x9F>', 'n', '.', '▁C', 'ả', 'm', '▁gi', 'ác', '▁h', 'ạ', 'n', 'h', '▁ph', 'ú', 'c', '▁n', 'h', 'ư', '▁n', 'h', 'ữ', 'ng', '▁b', 'ô', 'ng', '▁ho', 'a', '▁', 'đ', 'ua', '▁n', '<0xE1>', '<0xBB>', '<0x9F>', '▁tr', 'ong', '▁', 'án', 'h', '▁n', 'ắ', 'ng', '▁', 'ấ', 'm', '▁á', 'p', '▁c', 'ủ', 'a', '▁m', 'ù', 'a', '▁x', 'u', 'ân', '.', '▁', 'Đ', 'i', 'ề', 'u', '▁quan', '▁tr', 'ọ', 'ng', '▁là', '▁ta', '▁ph', 'ả', 'i', '▁y', 'ê', 'u', '▁v', 'à', '▁tr', 'ân', '▁tr', 'ọ', 'ng', '▁cu', 'ộ', 'c', '▁s', 'ố', 'ng', '▁n', 'à', 'y', '.', '<0x0A>', 'The', '▁primary', '▁use', '▁of', '▁L', 'La', 'MA', '▁is', '▁research', '▁on', '▁large', '▁language', '▁models', ',', '▁including']\n",
            "Tokenized by Vietnamese-LLaMA tokenizer:['▁Trái', '▁đất', '▁tròn', '▁quay', ',', '▁mặt', '▁trời', '▁mọc', '▁ngàn', '▁dặm', '▁xa', ',', '▁đánh', '▁thức', '▁cuộc', '▁sống', '▁bình', '▁minh', '.', '▁Những', '▁cánh', '▁diều', '▁trắng', '▁bay', '▁cao', '▁t', 'rê', 'n', '▁bầu', '▁trời', ',', '▁gió', '▁êm', '▁đềm', '▁thổi', '▁qua', '▁biển', '▁cỏ', '▁xanh', '▁m', 'ơn', '▁mở', 'n', '.', '▁Cảm', '▁giác', '▁hạnh', '▁phúc', '▁như', '▁những', '▁bông', '▁hoa', '▁đua', '▁nở', '▁trong', '▁ánh', '▁nắng', '▁ấm', '▁áp', '▁của', '▁mùa', '▁xuân', '.', '▁Điều', '▁quan', '▁trọng', '▁là', '▁ta', '▁phải', '▁yêu', '▁và', '▁trân', '▁trọng', '▁cuộc', '▁sống', '▁này', '.', '<0x0A>', 'The', '▁primary', '▁use', '▁of', '▁L', 'La', 'MA', '▁is', '▁research', '▁on', '▁large', '▁language', '▁models', ',', '▁including']\n",
            "---------------\n",
            "Tokenized by LLaMA tokenizer IDs: [1, 1605, 29976, 29875, 29871, 30128, 31145, 29873, 534, 30059, 29876, 439, 388, 29892, 286, 228, 189, 186, 29873, 534, 30997, 29875, 286, 30975, 29883, 8736, 24267, 270, 228, 189, 186, 29885, 921, 29874, 29892, 29871, 30128, 1715, 29882, 266, 31318, 29883, 2723, 30902, 29883, 269, 30984, 865, 289, 30097, 29876, 29882, 1375, 29882, 29889, 405, 29882, 31797, 865, 274, 1715, 29882, 652, 31343, 29884, 534, 31160, 865, 23041, 5777, 29877, 534, 5512, 289, 30884, 29884, 534, 30997, 29875, 29892, 330, 3556, 29871, 19553, 29871, 30128, 31343, 29885, 266, 228, 190, 152, 29875, 439, 29874, 4768, 31957, 29876, 274, 228, 190, 146, 921, 27731, 286, 30556, 29876, 286, 228, 190, 162, 29876, 29889, 315, 30643, 29885, 4005, 25968, 298, 30540, 29876, 29882, 1374, 30030, 29883, 302, 29882, 30416, 302, 29882, 31797, 865, 289, 30069, 865, 5089, 29874, 29871, 30128, 3357, 302, 228, 190, 162, 534, 549, 29871, 1715, 29882, 302, 31160, 865, 29871, 31145, 29885, 3976, 29886, 274, 31556, 29874, 286, 30071, 29874, 921, 29884, 10031, 29889, 29871, 30250, 29875, 31343, 29884, 29195, 534, 30975, 865, 18916, 11062, 1374, 30643, 29875, 343, 30037, 29884, 325, 30001, 534, 10031, 534, 30975, 865, 2723, 30902, 29883, 269, 30984, 865, 302, 30001, 29891, 29889, 13, 1576, 7601, 671, 310, 365, 5661, 1529, 338, 5925, 373, 2919, 4086, 4733, 29892, 3704]\n",
            "Tokenized by Vietnamese-LLaMA tokenizer IDs: [1, 34490, 32369, 34417, 33402, 29892, 32382, 33291, 35281, 33611, 36860, 33156, 29892, 32470, 32337, 32220, 32454, 32519, 32579, 29889, 32799, 33325, 36599, 33409, 23041, 32236, 260, 39093, 29876, 32980, 33291, 29892, 33723, 36228, 42257, 34797, 32228, 32543, 34502, 33245, 286, 32106, 32680, 29876, 29889, 36027, 33498, 33747, 33583, 32066, 32047, 34847, 33017, 33383, 35047, 32029, 33143, 33905, 33942, 32723, 32012, 33025, 34061, 29889, 33028, 29195, 32313, 18916, 11062, 32161, 32458, 32006, 34738, 32313, 32220, 32454, 32102, 29889, 13, 1576, 7601, 671, 310, 365, 5661, 1529, 338, 5925, 373, 2919, 4086, 4733, 29892, 3704]\n",
            "Test text:\n",
            " <unk>\n",
            "\n",
            "\n",
            "---------------\n",
            "Tokenized by LLaMA tokenizer:['<unk>', '<0x0A>', '<0x0A>']\n",
            "Tokenized by Vietnamese-LLaMA tokenizer:['<unk>', '<0x0A>', '<0x0A>']\n",
            "---------------\n",
            "Tokenized by LLaMA tokenizer IDs: [1, 0, 13, 13]\n",
            "Tokenized by Vietnamese-LLaMA tokenizer IDs: [1, 0, 13, 13]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate\n",
        "\n",
        "texts = [\"Cái cây cổ thụ ở trước cửa nhà đã tồn tại hơn một thế kỷ.\",\n",
        "\"Thị trấn nhỏ này nổi tiếng với bữa ăn đường phố ngon và giá cả phải chăng.\",\n",
        "\"Người dân đang tụ tập tại sân trường để chào đón đội bóng chiến thắng.\",\n",
        "\"Sự hiểu biết và tôn trọng đối với người khác là cơ sở của hòa bình xã hội.\",\n",
        "\"Vào mùa đông, tuyết rơi dày đặc và biến cảnh quang trường thành một cảnh trắng xóa.\",\n",
        "\"Cuộc sống nông thôn thường được đánh giá cao vì sự bình yên và gần gũi với thiên nhiên.\",\n",
        "\"Người dân địa phương nồng nàn mừng xuân bằng những lễ hội truyền thống.\",\n",
        "\"Trong tương lai, con người cần phải bảo vệ môi trường để bảo vệ tương lai cho thế hệ sau.\",\n",
        "\"Trong bức tranh này, một ngọn núi cao nằm dưới bầu trời xanh.\",\n",
        "\"Những đứa trẻ vui đùa trong công viên vào buổi chiều mặt trời.\",\n",
        "\"Đêm đã đến, và ngôi làng nhỏ im lặng dưới ánh trăng sáng.\",\n",
        "\"Người dân đang tận hưởng một buổi hội họp tại quảng trường trung tâm.\",\n",
        "\"Cuộc sống là một cuộc hành trình, hãy tận hưởng từng khoảnh khắc.\",\n",
        "\"Thiết kế và nghệ thuật đồ họa thường đòi hỏi sự sáng tạo và tư duy.\",\n",
        "\"Mùa thu đến, lá cây bắt đầu chuyển màu và rơi xuống đất.\",\n",
        "\"Công việc khó khăn luôn đòi hỏi sự kiên nhẫn và nỗ lực.\"]"
      ],
      "metadata": {
        "id": "BO5axFOdR0dM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results = []\n",
        "for text in texts:\n",
        "    results.append(len(llama_tokenizer.encode(text)))\n",
        "\n",
        "# Tokens per sentence\n",
        "sum(results) / len(results), results"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CFXn703xSGyo",
        "outputId": "33ee87ad-7a51-4587-f531-d15dcafa2fa3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(49.5625, [51, 53, 47, 54, 60, 58, 50, 54, 40, 45, 44, 49, 49, 50, 43, 46])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Merges x3 ??\n",
        "\n",
        "results = []\n",
        "for text in texts:\n",
        "    results.append(len(vietnamese_llama_tokenizer.encode(text)))\n",
        "\n",
        "\n",
        "# Tokens per sentence\n",
        "sum(results) / len(results), results"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3u8yZWl8SMXy",
        "outputId": "c30583e4-45a4-4c36-924a-8494ef572a5d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(17.6875, [17, 19, 17, 20, 20, 21, 16, 23, 16, 15, 16, 16, 16, 18, 18, 15])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    }
  ]
}